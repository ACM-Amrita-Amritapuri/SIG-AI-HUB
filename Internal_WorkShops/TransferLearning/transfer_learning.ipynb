{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e340f3-9d7c-4c37-9571-c44d8f2c4d16",
   "metadata": {},
   "source": [
    "## Image classification of MNISTfashion dataset with importing ResNet pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d0a2d-29c4-4293-a193-9b35fe59d31e",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d3c5fb22-087f-4ece-9261-4a65b38e3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4d1a03f9-4ee0-4650-b002-75ea9eb272d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafaf5e0-56c8-4702-be2c-74509b17c8d2",
   "metadata": {},
   "source": [
    "### Transformation for FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c9483a9-e580-40cd-b0e1-09d6b07feaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  \n",
    "    transforms.Resize((224, 224)),                \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f806107-d62b-40e4-be0f-a074d7aeb0c0",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e2ce8fd-5d12-4fa9-b83a-ab0fbc64c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Limitung the size of dataset for faster processing (subset of 1000 for training and 500 for testing)\n",
    "train_dataset.data = train_dataset.data[:1000]  \n",
    "train_dataset.targets = train_dataset.targets[:1000]\n",
    "\n",
    "test_dataset.data = test_dataset.data[:500]  \n",
    "test_dataset.targets = test_dataset.targets[:500]\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce05b43-d656-473f-ad06-4e3d6a5119b7",
   "metadata": {},
   "source": [
    "### Loading the pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1f98ca63-ffb3-4ab2-adf0-290f1dc449ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95905083-999f-4f9b-8d57-3b456040354f",
   "metadata": {},
   "source": [
    "### Freezing all layers (for feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c23a5391-c9db-46ac-b13d-8f36e13314ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fc890-0101-4651-bb7a-67453cd92224",
   "metadata": {},
   "source": [
    "#### Removing final fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b54c46f0-a589-42fe-ae91-b6dc575fb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*list(model.children())[:-1])  \n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf95c9a-3a2f-4d63-a9fd-50a751c2aa2a",
   "metadata": {},
   "source": [
    "#### Function to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "843ba783-a46a-4cbf-bfc8-6b0c36803c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader):\n",
    "    model.eval()  \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)  \n",
    "            outputs = outputs.view(outputs.size(0), -1)  \n",
    "            features.append(outputs.cpu())  \n",
    "            labels.append(targets)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    features = torch.cat(features)\n",
    "    labels = torch.cat(labels)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c677b-a9f3-4214-8e87-e3bf041428b9",
   "metadata": {},
   "source": [
    "#### Extracting features from the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d589aacc-312a-478d-b212-d9032e4de961",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_loader)\n",
    "test_features, test_labels = extract_features(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db35e41-ebcb-4f39-af2c-0a08decde66d",
   "metadata": {},
   "source": [
    "#### Simple classifier to train on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "158d0b25-9ac3-4489-8d29-484d59e52bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Linear(train_features.size(1), 10)  \n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd3014-eb94-4ea6-9a73-9163020f17c9",
   "metadata": {},
   "source": [
    "### Defining optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8c3ddf0a-9f1a-44d0-a222-64f97fda230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059245d-ecf6-424b-8722-94a041e92717",
   "metadata": {},
   "source": [
    "### Training the classifier on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a85bd38a-7420-497f-8952-9cc3b8cf3f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.9976, Accuracy: 37.70%\n",
      "Epoch [2/5], Loss: 1.2048, Accuracy: 74.40%\n",
      "Epoch [3/5], Loss: 0.8545, Accuracy: 80.10%\n",
      "Epoch [4/5], Loss: 0.6981, Accuracy: 82.40%\n",
      "Epoch [5/5], Loss: 0.6109, Accuracy: 83.20%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    correct = (predicted == labels).sum().item() \n",
    "    return correct\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train() \n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    indices = torch.randperm(train_features.size(0))\n",
    "    train_features = train_features[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "    \n",
    "    for i in range(0, len(train_features), 64):\n",
    "        batch_features = train_features[i:i+64].to(device)\n",
    "        batch_labels = train_labels[i:i+64].to(device)\n",
    "        \n",
    "        outputs = classifier(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        total_correct += calculate_accuracy(outputs, batch_labels)\n",
    "        total_samples += batch_labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = total_correct / total_samples * 100  \n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7ef14ece-694c-4199-a99d-c7dbd89aff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.20%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(classifier, features, labels):\n",
    "    classifier.eval()  # Set classifier to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        outputs = classifier(features.to(device))  # Make predictions\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
    "        correct = (predicted == labels.to(device)).sum().item()  # Compare with actual labels\n",
    "        total = labels.size(0)  # Get the total number of samples\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = calculate_accuracy(classifier, test_features, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f040f8-f75a-4225-bb55-f9e2c94edff9",
   "metadata": {},
   "source": [
    "### Tkinter window for displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a30856bc-b9d2-4310-ad2e-ea6470fee4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = tk.Tk()\n",
    "window.title(\"FashionMNIST Prediction\")\n",
    "\n",
    "image_label = tk.Label(window)\n",
    "image_label.pack()\n",
    "prediction_label = tk.Label(window, text=\"\")\n",
    "prediction_label.pack()\n",
    "\n",
    "test_images_raw = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    test_images_raw.append((images, labels))\n",
    "    break \n",
    "\n",
    "image_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cbae24-97c2-4926-84f0-3df26d428a12",
   "metadata": {},
   "source": [
    "### Function to make predictions and update GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "de5d3ac1-8f61-46c9-b3e6-ee0debd547c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prediction(image_index):\n",
    "    classifier.eval()\n",
    "    \n",
    "\n",
    "    image = test_images_raw[0][0][image_index]  \n",
    "    feature = test_features[image_index].unsqueeze(0).to(device)  \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = classifier(feature)\n",
    "        _, predicted_label = torch.max(output, 1)\n",
    "    \n",
    "    # Map FashionMNIST label indices to human-readable names\n",
    "    label_map = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    predicted_name = label_map[predicted_label.item()]\n",
    "    \n",
    "    # Convert the image to PIL format for Tkinter display\n",
    "    image = image.cpu().numpy().transpose(1, 2, 0)  # Convert from Tensor to numpy\n",
    "    image = (image * 0.5 + 0.5) * 255  \n",
    "    image = Image.fromarray(image.astype('uint8'))\n",
    "\n",
    "    \n",
    "    img_tk = ImageTk.PhotoImage(image)\n",
    "    image_label.config(image=img_tk)\n",
    "    image_label.image = img_tk \n",
    "    prediction_label.config(text=f\"Predicted: {predicted_name}\")\n",
    "\n",
    "\n",
    "def next_image():\n",
    "    global image_index\n",
    "    image_index = (image_index + 1) % len(test_images_raw[0][0])  \n",
    "    update_prediction(image_index)\n",
    "\n",
    "\n",
    "next_button = tk.Button(window, text=\"Next Image\", command=next_image)\n",
    "next_button.pack()\n",
    "\n",
    "\n",
    "update_prediction(image_index)\n",
    "\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f77f574-ea1a-41ee-87b9-734855dbd03b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
